nohup: 忽略输入
/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Using RTX 3090 or 4000 series which doesn't support faster communication speedups. Ensuring P2P and IB communications are disabled.
/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
(INFO) 2024-12-17 14:21:05,586 - MultiTask_train_logger:
Args Info:
--task_name MultiTask_train
--n_save_step 3
--output_dir ./output
--task_type ada
--accelertor_cfg.log_with None
--accelertor_cfg.gradient_accumulation_steps 16
--base_dateset_cfg.data_path /home/smliu/datasets/instruct/bigbench
--base_dateset_cfg.sub_data_path None
--base_dateset_cfg.tokenizer_pretrain_path /home/smliu/huggingface/bit-dny/MindLLM-1b3-chat-zh-v2.0
--base_dateset_cfg.max_len 512
--base_dateset_cfg.remove_chinese True
--base_dateset_cfg.padding_side left
--base_dateset_cfg.prompt_only False
--base_dateset_cfg.tokenize_type prompt_response
--dataset_data_paths ['/home/smliu/datasets/instruct/bigbench/ascii_word_recognition', '/home/smliu/datasets/instruct/bigbench/auto_categorization', '/home/smliu/datasets/instruct/bigbench/auto_debugging', '/home/smliu/datasets/instruct/bigbench/bridging_anaphora_resolution_barqa', '/home/smliu/datasets/instruct/bigbench/chess_state_tracking', '/home/smliu/datasets/instruct/bigbench/chinese_remainder_theorem', '/home/smliu/datasets/instruct/bigbench/codenames', '/home/smliu/datasets/instruct/bigbench/conlang_translation', '/home/smliu/datasets/instruct/bigbench/cryptonite', '/home/smliu/datasets/instruct/bigbench/disfl_qa', '/home/smliu/datasets/instruct/bigbench/few_shot_nlg', '/home/smliu/datasets/instruct/bigbench/gem', '/home/smliu/datasets/instruct/bigbench/gender_inclusive_sentences_german', '/home/smliu/datasets/instruct/bigbench/hindi_question_answering', '/home/smliu/datasets/instruct/bigbench/international_phonetic_alphabet_transliterate', '/home/smliu/datasets/instruct/bigbench/language_games', '/home/smliu/datasets/instruct/bigbench/linguistic_mappings', '/home/smliu/datasets/instruct/bigbench/linguistics_puzzles', '/home/smliu/datasets/instruct/bigbench/list_functions', '/home/smliu/datasets/instruct/bigbench/matrixshapes', '/home/smliu/datasets/instruct/bigbench/minute_mysteries_qa', '/home/smliu/datasets/instruct/bigbench/modified_arithmetic', '/home/smliu/datasets/instruct/bigbench/mult_data_wrangling', '/home/smliu/datasets/instruct/bigbench/object_counting', '/home/smliu/datasets/instruct/bigbench/operators', '/home/smliu/datasets/instruct/bigbench/paragraph_segmentation', '/home/smliu/datasets/instruct/bigbench/parsinlu_reading_comprehension', '/home/smliu/datasets/instruct/bigbench/physics_questions', '/home/smliu/datasets/instruct/bigbench/polish_sequence_labeling', '/home/smliu/datasets/instruct/bigbench/qa_wikidata', '/home/smliu/datasets/instruct/bigbench/repeat_copy_logic', '/home/smliu/datasets/instruct/bigbench/rephrase', '/home/smliu/datasets/instruct/bigbench/scientific_press_release', '/home/smliu/datasets/instruct/bigbench/semantic_parsing_in_context_sparc', '/home/smliu/datasets/instruct/bigbench/semantic_parsing_spider', '/home/smliu/datasets/instruct/bigbench/simp_turing_concept', '/home/smliu/datasets/instruct/bigbench/simple_arithmetic_json', '/home/smliu/datasets/instruct/bigbench/simple_text_editing', '/home/smliu/datasets/instruct/bigbench/sufficient_information', '/home/smliu/datasets/instruct/bigbench/tellmewhy', '/home/smliu/datasets/instruct/bigbench/tense', '/home/smliu/datasets/instruct/bigbench/topical_chat', '/home/smliu/datasets/instruct/bigbench/unnatural_in_context_learning', '/home/smliu/datasets/instruct/bigbench/word_sorting', '/home/smliu/datasets/instruct/bigbench/word_unscrambling']
--model_cfg.model_pretrain_path /home/smliu/huggingface/bit-dny/MindLLM-1b3-chat-zh-v2.0
--model_cfg.model_class None
--model_cfg.peft_cfg.adapter_name lora-altered
--model_cfg.peft_cfg.target_modules ['q_proj', 'k_proj', 'v_proj', 'out_proj']
--model_cfg.peft_cfg.r 48
--model_cfg.peft_cfg.lora_alpha 32
--model_cfg.peft_cfg.lora_dropout 0.0
--model_cfg.peft_cfg.pref_dim 45
--model_cfg.peft_cfg.pref_r 1
--model_cfg.ckpt_path ./ckpts
--model_cfg.use_ckpt False
--model_cfg.ckpt_load_path None
--model_cfg.device cuda
--model_cfg.max_len 512
--manipulator_cfg.weighted_loss_type mols
--manipulator_cfg.svd_lora_type None
--manipulator_cfg.svd_lora_random_init None
--manipulator_cfg.svd_lora_split_percentage 0.125
--manipulator_cfg.n_adapt_step 128
--lr 0.0001
--weight_decay 0.0005
--n_episode 1
--train_batch_size 1
--val_batch_size 8

[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/smliu/llm-adamtl/train_multitask.py", line 225, in <module>
[rank1]:     main()
[rank1]:   File "/home/smliu/llm-adamtl/train_multitask.py", line 209, in main
[rank1]:     trainer = MTL_Trainer(
[rank1]:   File "/home/smliu/llm-adamtl/train_multitask.py", line 28, in __init__
[rank1]:     super().__init__(
[rank1]:   File "/home/smliu/llm-adamtl/base_trainer.py", line 42, in __init__
[rank1]:     self.accelerator = Accelerator(
[rank1]:   File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/accelerate/accelerator.py", line 380, in __init__
[rank1]:     self.state = AcceleratorState(
[rank1]:   File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/accelerate/state.py", line 761, in __init__
[rank1]:     PartialState(cpu, **kwargs)
[rank1]:   File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/accelerate/state.py", line 233, in __init__
[rank1]:     torch.cuda.set_device(self.device)
[rank1]:   File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/cuda/__init__.py", line 420, in set_device
[rank1]:     torch._C._cuda_setDevice(device)
[rank1]: RuntimeError: CUDA error: invalid device ordinal
[rank1]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank1]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank1]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[rank1]:[W1217 14:21:06.873573065 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1217 14:21:06.662000 140021187252608 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2112347 closing signal SIGTERM
E1217 14:21:06.777000 140021187252608 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 2112348) of binary: /home/smliu/anaconda3/envs/rlhf/bin/python
Traceback (most recent call last):
  File "/home/smliu/anaconda3/envs/rlhf/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1014, in launch_command
    multi_gpu_launcher(args)
  File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/smliu/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/smliu/llm-adamtl/train_multitask.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-17_14:21:06
  host      : node237
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2112348)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
